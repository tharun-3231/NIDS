# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from ibm_watson_machine_learning import APIClient
import joblib
import requests
import json

# Load the dataset (replace with your actual dataset path)
# Dataset from Kaggle: https://www.kaggle.com/datasets/sampadab17/networkintrusion-detection
try:
    df = pd.read_csv('network_intrusion_data.csv')  # Update with your actual filename
except FileNotFoundError:
    print("Please download the dataset from Kaggle and update the file path")
    exit()

# Data Preprocessing
print("Dataset shape:", df.shape)
print("\nFirst few rows:")
print(df.head())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

# Drop rows with missing values if any
df = df.dropna()

# Convert categorical features to numerical
label_encoders = {}
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if col != 'class':  # We'll handle the target separately
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

# Encode target variable
le_target = LabelEncoder()
df['class'] = le_target.fit_transform(df['class'])

# Feature selection (you may need to adjust based on your dataset)
X = df.drop('class', axis=1)
y = df['class']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Training - Random Forest Classifier
print("\nTraining Random Forest Classifier...")
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Model Evaluation
y_pred = rf_classifier.predict(X_test)

print("\nModel Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le_target.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le_target.classes_, 
            yticklabels=le_target.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Save the model and preprocessing objects
joblib.dump(rf_classifier, 'nids_rf_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(label_encoders, 'label_encoders.pkl')
joblib.dump(le_target, 'target_encoder.pkl')

# IBM Cloud Deployment (similar to the attached notebook example)
def deploy_to_ibm_cloud():
    # Authenticate with IBM Cloud
    wml_credentials = {
        "url": "https://eu-gb.ml.cloud.ibm.com",
        "apikey": "YOUR_API_KEY",  # Replace with your IBM Cloud API key
        "instance_id": "YOUR_INSTANCE_ID"  # Replace with your instance ID
    }
    
    client = APIClient(wml_credentials)
    
    # Set up the deployment space
    space_name = "nids_deployment"
    space_id = None
    
    try:
        space_details = client.spaces.get_details()
        for space in space_details['resources']:
            if space['entity']['name'] == space_name:
                space_id = space['metadata']['id']
                break
        
        if not space_id:
            space_meta = {
                "name": space_name,
                "description": "Space for Network Intrusion Detection System"
            }
            space = client.spaces.store(space_meta)
            space_id = client.spaces.get_id(space)
        
        client.set.default_space(space_id)
    except Exception as e:
        print("Error setting up space:", str(e))
        return
    
    # Save the model to IBM Cloud
    software_spec_uid = client.software_specifications.get_id_by_name("runtime-22.1-py3.9")
    
    model_meta = {
        client.repository.ModelMetaNames.NAME: "Network_Intrusion_Detection_Model",
        client.repository.ModelMetaNames.TYPE: "scikit-learn_1.0",
        client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid
    }
    
    try:
        saved_model = client.repository.store_model(
            model="nids_rf_model.pkl",
            meta_props=model_meta,
            training_data=X_train,
            training_target=y_train
        )
        
        model_uid = client.repository.get_model_id(saved_model)
        print("Model saved with UID:", model_uid)
        
        # Create deployment
        deployment_meta = {
            client.deployments.ConfigurationMetaNames.NAME: "NIDS Deployment",
            client.deployments.ConfigurationMetaNames.ONLINE: {}
        }
        
        deployment = client.deployments.create(model_uid, meta_props=deployment_meta)
        deployment_uid = client.deployments.get_uid(deployment)
        print("Deployment created with UID:", deployment_uid)
        
    except Exception as e:
        print("Error deploying model:", str(e))

# Uncomment to deploy to IBM Cloud
# deploy_to_ibm_cloud()

# Function to make predictions (for demonstration)
def predict_intrusion(sample_data):
    # Load the saved model and preprocessing objects
    model = joblib.load('nids_rf_model.pkl')
    scaler = joblib.load('scaler.pkl')
    label_encoders = joblib.load('label_encoders.pkl')
    le_target = joblib.load('target_encoder.pkl')
    
    # Preprocess the sample data
    sample_df = pd.DataFrame([sample_data])
    
    # Apply label encoding to categorical features
    for col in label_encoders:
        if col in sample_df.columns:
            sample_df[col] = label_encoders[col].transform([sample_data[col]])
    
    # Scale the features
    sample_scaled = scaler.transform(sample_df)
    
    # Make prediction
    prediction = model.predict(sample_scaled)
    probability = model.predict_proba(sample_scaled)
    
    # Convert prediction back to original label
    predicted_class = le_target.inverse_transform(prediction)
    
    # Get probabilities for each class
    class_probabilities = dict(zip(le_target.classes_, probability[0]))
    
    return predicted_class[0], class_probabilities

# Example usage (with sample data matching your dataset features)
sample_data = {
    'duration': 0,
    'protocol_type': 'tcp',  # Will be encoded
    'service': 'http',       # Will be encoded
    'flag': 'SF',            # Will be encoded
    'src_bytes': 215,
    'dst_bytes': 45076,
    # Add all other features from your dataset
}

# Uncomment to test prediction
# predicted_class, probabilities = predict_intrusion(sample_data)
# print(f"Predicted class: {predicted_class}")
# print("Class probabilities:", probabilities)